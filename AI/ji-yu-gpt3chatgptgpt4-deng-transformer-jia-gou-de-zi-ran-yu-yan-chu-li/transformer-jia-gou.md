---
description: Attention is All You Need
---

# Transformer 架构

RNN：循环神经网络，一种用于处理序列数据的神经网络结构，它是一种连接模型，**通过循环节点网络来捕捉序列的动态**。

{% hint style="info" %}
RNN的核心结构包括一个隐藏状态向量，该向量随着输入序列的每个时间步进行更新，从而在不同时间步传递信息。通过这种机制，RNN能够捕捉序列中的长短时依赖关系，并在自然语言处理、语音识别、时间序列预测等任务中展现出优异的性能
{% endhint %}

> 循环神经网络 (RNN) 和前馈神经网络 (Feedforward Neural Network) 是两种不同类型的神经网络
>
> 前馈神经网络是一种最简单的神经网络，其特点是信息从输入层流向输出层，不存在循环连接（独立性假设）。使得机器学习取得了很大的进展，适用于解决简单的回归问题或分类问题

<details>

<summary>RNN的局限性 / Transformer的优势</summary>

* **破除"串行计算"瓶颈（解决算力利用率极低的问题）**
  *   **痛点**：RNN 的计算依赖时序，必须算完第 t−1 个词，才能算第 t 个词。这种 O(N)

      的串行依赖导致模型无法在 GPU 上进行大规模并行矩阵运算。
  * **解决**：Transformer 的自注意力机制允许**整个序列的所有 Token 同时进行计算**，彻底打破了时间步的耦合，实现了高度并行化，极大地提升了训练吞吐量，为后来大模型的“暴力美学”（Scaling Law）奠定了工程基础。
* **克服"长程依赖（Long-term Dependency）"衰减（解决遗忘问题）**
  * **痛点**：在 RNN 中，句子开头的词语信息在经过数十步的传递后，会由于"梯度消失/爆炸"而严重衰减。
  * **解决**：Transformer 通过注意力矩阵，让序列中任意两个 Token 之间都能直接发生"连接（点积）"。这意味着**任意两个词之间的信息传递路径长度都被缩短为** O(1)，彻底解决了长文本中早期信息丢失的弊端。

</details>

***

<figure><img src="../.gitbook/assets/image (3).png" alt="" width="375"><figcaption><p>原始 Transformer 模型架构</p></figcaption></figure>

左侧是6层的编码器堆叠，右侧是6层的解码器堆叠。

输入进入左侧的编码器，穿过多头自注意力子层以及前馈神经网络子层，然后其目标输出进入右侧解码器的多头注意力子层和前馈神经网络子层



注意力取代了随着两个单词的距离增加而需要增加参数的循环函数（RNN）。它是"单词到单词"的操作，即 token 到 token（词元）的操作。**注意力机制将找出每个单词与序列中所有单词的相关性**。

> 注意力将单词向量进行**点积操作**以得出一个单词与所有单词的关系，包括与自身的关系
>
>
>
> 点积操作：先求两数字序列中每组对应元素的积，再求所有积之和，结果即&#x4E3A;_&#x70B9;积_

***

## 编码器堆叠

原始 Transformer 模型的编码器由结构相同的 6 层堆叠而成（N=6）。

每层以这两个子层为主：一个多头注意力子层和一个前馈神经网络子层。每一个子层周围都有一条指向层规范化的残差连接，这些连接将子层的未处理输入 x 传给层规范化函数（确保位置编码等关键信息不会在中途丢失），规范化输出为：输出都有一个很定的维度 d<sub>model</sub>=512

$$
LayerNormalization(x+Sublayer(x))
$$

每层先从前一层学习，然后从不同角度探索序列中词元的相关性

关键操作为点积操作，保持恒定的维度可以减少计算的操作次数，并且更容易跟踪通过模型传输的信息

***

### 输入嵌入

输入嵌入子层将输入词元转换为维度 d<sub>model</sub>=512 的特征向量。工作方式为：<mark style="color:blue;">通过词元分析器（tokenizer）将句子拆分为词元</mark>。词元化方法包括 BPE（原始 Transformer 使用）、WordPiece 和 SentencePiece

假设嵌入句子：

```
The black cat sat on the couch and the brown dog slept on the rug.
```

black 和 brown 这两个词的嵌入向量应该是相似的，black 向量：

```
black=[[-0.0120671, 0.11632373...]] #512
```

可以使用余弦相似度来查看单词的嵌入是否相似

> 余弦相似度：欧几里得范数（L2范数）在单位球面创建向量

***

### 位置编码

Transformer 需要需要知道单词在序列中的位置，所以通过单位球面求正弦值（偶数）和余弦值（奇数）来表示位置编码。

将位置编码添加进嵌入向量：y1为black的嵌入向量，位置向量为pe(2)。这里避免pe(2)的值过大，导致丢失嵌入信息，所以需要加大y1（比如y1\*math.sqrt(d\_model)）

$$
pc(black)=y1 + pe(2)
$$

***

### 子层1：多头注意力子层

编码器堆叠第一层的多头注意力子层的输入是一个向量（包含单词嵌入和位置编码信息），堆叠的下一层不会再重复这些操作

每分析一次d<sub>model</sub>，只能得到一个观点（单词1与单词2的相关性）。所以需要进行并行计算，将d<sub>model</sub>=512维分成8块（d<sub>k</sub>=64维）

并行运行8个“头”以加快训练速度，获得8个不同的、体现每个单词与另一个单词关系的表示子空间。这8个头的输出连接成一个大的矩阵 Z，得到了整个多注意力机制的最终输出

***

### 子层2：前馈神经网络子层（略）

* FFN 是全连接神经网络
* FFN 是逐位置处理的，每个位置的数据都单独进行相同的处理
* FFN 包含两个隐藏层，并应用 ReLU 激活函数

***

## 解码器堆叠

### 掩码注意力层

Transformer 是一个自回归模型。它使用前面的输出序列作为附加输入。解码器的多头注意力层适用于编码器相同的过程。但是因为掩码了后续部分，<mark style="color:red;">掩码多头注意力子层 1 仅能将注意力应用于当前位置之前的位置（包括当前位置）。后续部分的单词对 Transformer 是不可见的，从而迫使它学习如何预测</mark>

### 线性层

线性层使用一个线性函数生成一个输出序列，线性函数根据模型而异，但不会偏离以下方法：

$$
y=w*x + b
$$

w和b都是可学习的参数

因此，<mark style="color:blue;">**线性层将生成序列的下一个可能元素，然后 softmax 函数将其转换为一个概率元素**</mark>
