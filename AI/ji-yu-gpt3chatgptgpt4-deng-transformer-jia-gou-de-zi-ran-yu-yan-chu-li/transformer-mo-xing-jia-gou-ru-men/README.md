# Transformer模型架构入门

RNN：循环神经网络，一种用于处理序列数据的神经网络结构，它是一种连接模型，**通过循环节点网络来捕捉序列的动态**。

{% hint style="info" %}
RNN的核心结构包括一个隐藏状态向量，该向量随着输入序列的每个时间步进行更新，从而在不同时间步传递信息。通过这种机制，RNN能够捕捉序列中的长短时依赖关系，并在自然语言处理、语音识别、时间序列预测等任务中展现出优异的性能
{% endhint %}

> 循环神经网络 (RNN) 和前馈神经网络 (Feedforward Neural Network) 是两种不同类型的神经网络
>
> 前馈神经网络是一种最简单的神经网络，其特点是信息从输入层流向输出层，不存在循环连接（独立性假设）。使得机器学习取得了很大的进展，适用于解决简单的回归问题或分类问题

<details>

<summary>RNN的局限性 / Transformer的优势</summary>

* **破除“串行计算”瓶颈（解决算力利用率极低的问题）**
  *   **痛点**：RNN 的计算依赖时序，必须算完第 t−1 个词，才能算第 t 个词。这种 O(N)

      的串行依赖导致模型无法在 GPU 上进行大规模并行矩阵运算。
  * **解决**：Transformer 的自注意力机制允许**整个序列的所有 Token 同时进行计算**，彻底打破了时间步的耦合，实现了高度并行化，极大地提升了训练吞吐量，为后来大模型的“暴力美学”（Scaling Law）奠定了工程基础。
* **克服“长程依赖（Long-term Dependency）”衰减（解决遗忘问题）**
  * **痛点**：在 RNN 中，句子开头的词语信息在经过数十步的传递后，会由于“梯度消失/爆炸”而严重衰减。
  * **解决**：Transformer 通过注意力矩阵，让序列中任意两个 Token 之间都能直接发生“连接（点积）”。这意味着**任意两个词之间的信息传递路径长度都被缩短为** O(1)，彻底解决了长文本中早期信息丢失的弊端。

</details>

