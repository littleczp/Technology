# BERT 的架构

BERT 将双向注意力机制引入 Transformer 模型中

## 编码器堆叠

BERT 只使用了 Transformer 编码器部分，没有使用解码器部分。

