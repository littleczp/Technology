---
description: Attention is All You Need
---

# Transformer 架构

<figure><img src="../.gitbook/assets/image (3).png" alt="" width="375"><figcaption><p>原始 Transformer 模型架构</p></figcaption></figure>

左侧是6层的编码器堆叠，右侧是6层的解码器堆叠。

输入进入左侧的编码器，穿过多头自注意力子层以及前馈神经网络子层，然后其目标输出进入右侧解码器的多头注意力子层和前馈神经网络子层



注意力取代了随着两个单词的距离增加而需要增加参数的循环函数（RNN）。它是“单词到单词”的操作，即 token 到 token（词元）的操作。**注意力机制将找出每个单词与序列中所有单词的相关性**。

> 注意力将单词向量进行**点积操作**以得出一个单词与所有单词的关系，包括与自身的关系
>
>
>
> 点积操作：先求两数字序列中每组对应元素的积，再求所有积之和，结果即&#x4E3A;_&#x70B9;积_

***

## 编码器堆叠

原始 Transformer 模型的编码器由结构相同的 6 层堆叠而成（N=6）。

每层以这两个子层为主：一个多头注意力子层和一个前馈神经网络子层。每一个子层周围都有一条指向层规范化的残差连接，这些连接将子层的未处理输入 x 传给层规范化函数（确保位置编码等关键信息不会在中途丢失），规范化输出为：输出都有一个很定的维度 d<sub>model</sub>=512

$$
LayerNormalization(x+Sublayer(x))
$$

每层先从前一层学习，然后从不同角度探索序列中词元的相关性

关键操作为点积操作，保持恒定的维度可以减少计算的操作次数，并且更容易跟踪通过模型传输的信息

***

### 输入嵌入

输入嵌入子层将输入词元转换为维度 d<sub>model</sub>=512 的特征向量。工作方式为：<mark style="color:blue;">通过词元分析器（tokenizer）将句子拆分为词元</mark>。词元化方法包括 BPE（原始 Transformer 使用）、WordPiece 和 SentencePiece

假设嵌入句子：The black cat sat on the couch and the brown dog slept on the rug.

black 和 brown 这两个词的嵌入向量应该是相似的，black 向量：

```
black=[[-0.0120671, 0.11632373...]] #512
```

可以使用余弦相似度来查看单词的嵌入是否相似

> 余弦相似度：欧几里得范数（L2范数）在单位球面创建向量

***

### 位置编码

Transformer 需要需要知道单词在序列中的位置，所以通过单位球面求正弦值（偶数）和余弦值（奇数）来表示位置编码。

将位置编码添加进嵌入向量：y1为black的嵌入向量，位置向量为pe(2)。这里避免pe(2)的值过大，导致丢失嵌入信息，所以需要加大y1（比如y1\*math.sqrt(d\_model)）

$$
pc(black)=y1 + pe(2)
$$

***

### 子层1：多头注意力子层

编码器堆叠第一层的多头注意力子层的输入是一个向量（包含单词嵌入和位置编码信息），堆叠的下一层不会再重复这些操作

每分析一次d<sub>model</sub>，只能得到一个观点（单词1与单词2的相关性）。所以需要进行并行计算，将d<sub>model</sub>=512维分成8块（d<sub>k</sub>=64维）

并行运行8个“头”以加快训练速度，获得8个不同的、体现每个单词与另一个单词关系的表示子空间。这8个头的输出连接成一个大的矩阵 Z，得到了整个多注意力机制的最终输出

***

### 子层2：前馈神经网络子层（略）

* FFN 是全连接神经网络
* FFN 是逐位置处理的，每个位置的数据都单独进行相同的处理
* FFN 包含两个隐藏层，并应用 ReLU 激活函数

***

## 解码器堆叠

### 掩码注意力层

Transformer 是一个自回归模型。它使用前面的输出序列作为附加输入。解码器的多头注意力层适用于编码器相同的过程。但是因为掩码了后续部分，<mark style="color:red;">掩码多头注意力子层 1 仅能将注意力应用于当前位置之前的位置（包括当前位置）。后续部分的单词对 Transformer 是不可见的，从而迫使它学习如何预测</mark>

### 线性层

线性层使用一个线性函数生成一个输出序列，线性函数根据模型而异，但不会偏离以下方法：

$$
y=w*x + b
$$

w和b都是可学习的参数

因此，<mark style="color:blue;">**线性层将生成序列的下一个可能元素，然后 softmax 函数将其转换为一个概率元素**</mark>
