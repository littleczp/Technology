---
description: Attention is All You Need
---

# Transformer 架构

<figure><img src="../.gitbook/assets/image (3).png" alt="" width="375"><figcaption><p>原始 Transformer 模型架构</p></figcaption></figure>

左侧是6层的编码器堆叠，右侧是6层的解码器堆叠。

输入进入左侧的编码器，穿过多头自注意力子层以及前馈神经网络子层，然后其目标输出进入右侧解码器的多头注意力子层和前馈神经网络子层



注意力取代了随着两个单词的距离增加而需要增加参数的循环函数（RNN）。它是“单词到单词”的操作，即 token 到 token（词元）的操作。**注意力机制将找出每个单词与序列中所有单词的相关性**。

> 注意力将单词向量进行**点积操作**以得出一个单词与所有单词的关系，包括与自身的关系
>
>
>
> 点积操作：先求两数字序列中每组对应元素的积，再求所有积之和，结果即&#x4E3A;_&#x70B9;积_

***

## 编码器堆叠

原始 Transformer 模型的编码器由结构相同的 6 层堆叠而成（N=6）。

每层以这两个子层为主：一个多头注意力子层和一个前馈神经网络子层。每一个子层周围都有一条指向层规范化的残差连接，这些连接将子层的未处理输入 x 传给层规范化函数（确保位置编码等关键信息不会在中途丢失），规范化输出为：输出都有一个很定的维度 d<sub>model</sub>=512

$$
LayerNormalization(x+Sublayer(x))
$$

每层先从前一层学习，然后从不同角度探索序列中词元的相关性

关键操作为点积操作，保持恒定的维度可以减少计算的操作次数，并且更容易跟踪通过模型传输的信息

***

## 输入嵌入

输入嵌入子层将输入词元转换为维度 d<sub>model</sub>=512 的特征向量。工作方式为：<mark style="color:blue;">通过词元分析器（tokenizer）将句子拆分为词元</mark>。词元化方法包括 BPE（原始 Transformer 使用）、WordPiece 和 SentencePiece

假设嵌入句子：The black cat sat on the couch and the brown dog slept on the rug.

black 和 brown 这两个词的嵌入向量应该是相似的，black 向量：

```
black=[[-0.0120671, 0.11632373...]] #512
```

可以使用余弦相似度来查看单词的嵌入是否相似

> 余弦相似度：欧几里得范数（L2范数）在单位球面创建向量

***

## 位置编码

Transformer 需要需要知道单词在序列中的位置，所以通过单位球面求正弦值（偶数）和余弦值（奇数）来表示位置编码。

将位置编码添加进嵌入向量：y1为black的嵌入向量，位置向量为pe(2)。这里避免pe(2)的值过大，导致丢失嵌入信息，所以需要加大y1（比如y1\*math.sqrt(d\_model)）

$$
pc(black)=y1 + pe(2)
$$

***

## 子层1：多头注意力子层

编码器堆叠第一层的多头注意力子层的输入是一个向量（包含单词嵌入和位置编码信息），堆叠的下一层不会再重复这些操作

每分析一次d<sub>model</sub>，只能得到一个观点（单词1与单词2的相关性）。所以需要进行并行计算，将d<sub>model</sub>=512维分成8块（d<sub>k</sub>=64维）

并行运行8个“头”以加快训练速度，获得8个不同的、体现每个单词与另一个单词关系的表示子空间。这8个头的输出连接成一个大的矩阵 Z，得到了整个多注意力机制的最终输出

***

